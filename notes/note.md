/*
    Generated by Claude 3.5
*/

# Note

## 基础架构
### 执行层级
- Hierarchy：Grid → Block → Warp → Thread
  - Grid：kernel 启动的所有 thread 集合
  - Block：在单个 SM 上执行的 thread 块
  - Warp：32个 thread 的基本执行单位
  - Thread：最小执行单元

- 数量关系（以 G80 为例）：
  - 每个 SM 最多 768 个 thread 
  - 每个 SM 最多 8 个 blocks
  - 每个 block 通常是 32 的倍数
  - 每个 warp 固定 32 个 thread 

### 硬件组件
- SM (Streaming Multiprocessor)
  - 管理和执行多个 blocks
  - 包含多个 SP 和共享资源
  - 例：G80 每个 SM 有 8 个 SP

- SP/CUDA Core
  - 实际执行计算的处理器
  - 每周期执行一个浮点运算
  - 处理来自不同 warps 的 thread 

### 特殊 Register 
- threadIdx：当前 thread 在 block 中的索引
- blockIdx：当前 block 在 grid 中的索引
- blockDim：block 的维度（ thread 数）
- gridDim：grid 的维度（block 数）

## 内存层次
### thread 级内存
-  Register
  - 每 thread 私有，最快
  - G80: 8K/SM，约10个/ thread 
  - 溢出会降低性能

- Local Memory
  - 存储在 Global Memory 
  - 用于 Register 溢出
  - **性能较差**，有 L2 Cache 会好一些

### Block 级内存
-  Shared Memory 
  - 每 block 共享
  - 低延迟，片上存储
  - G80: 16KB/SM，2KB/block

### Grid 级内存
-  Global Memory
  - 所有 thread 可访问
  - 高延迟（100+周期）
  - 带宽演进：G80(86GB/s) → RTX4090(1008GB/s)

-  Constant Memory
  - 只读，64KB 上限
  - 适合广播访问
  - 所有 thread 读相同位置最优

## 变量声明
| 类型 | 内存 | 作用域 | 生命周期 |
|-----|------|--------|----------|
| 自动变量 |  Register  |  thread  | kernel |
| 数组变量 |  Loacl Memory  |  thread  | kernel |
| __shared__ |  Shared Memory  | block | kernel |
| __device__ |  Global Memory  | grid | 应用程序 |
| __constant__ |  Constant Memory  | grid | 应用程序 |

## 性能优化
### 延迟隐藏
- 原理：保持足够 warps 活跃
- 实际例子：
  ```
  内存访问延迟：200 周期
  每个 warp 的计算时间：4 操作 × 4 周期 = 16 周期
  所需 warp 数：ceil(200/16) = 13 warps
  ```

### 同步注意事项
- __syncthreads() 使用规范
- 只能在同一 block 内同步
- 保持 thread 执行时间均衡

## 内存传输优化
### 带宽瓶颈
- GPU 内部内存带宽
  - 非常高：如 1080 Ti 达到 330 GB/s
  - 是优化性能的关键因素

- Host-Device 传输带宽
  - 受 PCIe 带宽限制
  - PCIe 3.0：每通道约 1GB/s
  - 16通道理论带宽：~16 GB/s
  - 实际带宽：~13 GB/s
  - 实测通常只有 4-6 GB/s
  - **比设备内部带宽慢 55+ 倍！**

### Pageable vs Pinned Memory
- Pageable Memory（默认）
  ```cpp
  // 普通分配（可分页）
  float *h_Pageable = (float*)malloc(bytes);  // host pageable
  ```
  - CPU 默认分配的是可分页内存
  - **GPU 无法直接访问**
  - 需要额外的内存拷贝开销

- Pinned Memory（锁页内存）
  ```cpp
  // 锁页内存分配
  float *h_Pinned;
  CUDA_CHECK(cudaMallocHost((void**)&h_Pinned, bytes));  // host pinned
  ```
  - 使用 cudaMallocHost 分配
  - 内存**固定**在物理地址
  - 可以实现更高的传输带宽
  - 需要注意错误检查

### 性能对比
- 传输带宽（以 GTX 770 为例）：
  ```
  Pageable 传输：
  - Host → Device: 4.60 GB/s
  - Device → Host: 4.64 GB/s
  
  Pinned 传输：
  - Host → Device: 12.31 GB/s
  - Device → Host: 12.58 GB/s
  ```

### 最佳实践
- 优化策略：
  - 减少 Host-Device 传输次数
  - 优先使用 Pinned Memory
  - 批量传输代替频繁小传输
  - 在可能的情况下重用设备数据

- 注意事项：
  - Pinned Memory 分配可能失败
  - 过度使用可能影响系统性能
  - 及时释放不需要的 Pinned Memory
  - 使用 CUDA_CHECK 进行错误处理

## 线程索引和维度选择
### 线程索引计算
- 1D 索引计算（最常用）
  ```cpp
  // 1D 线性索引
  int idx = blockIdx.x * blockDim.x + threadIdx.x;
  ```

- 2D 索引计算
  ```cpp
  // 2D索引计算 - 分步方式
  // 1. 先计算x和y的一维索引
  int x = blockIdx.x * blockDim.x + threadIdx.x;
  int y = blockIdx.y * blockDim.y + threadIdx.y;
  
  // 2. 将二维坐标转换为一维线性索引
  int idx = y * (gridDim.x * blockDim.x) + x;  // y * width + x
  ```

- 3D 索引计算
  ```cpp
  // 3D索引计算 - 分步方式
  // 1. 先计算x、y、z的一维索引
  int x = blockIdx.x * blockDim.x + threadIdx.x;
  int y = blockIdx.y * blockDim.y + threadIdx.y;
  int z = blockIdx.z * blockDim.z + threadIdx.z;
  
  // 2. 将三维坐标转换为一维线性索引
  int width = gridDim.x * blockDim.x;    // x方向的总长度
  int height = gridDim.y * blockDim.y;    // y方向的总长度
  int idx = z * (width * height) + y * width + x;  
  // z * (width * height) + y * width + x
  ```

### 维度选择建议
- 1D 适用场景
  - 线性数组处理
  - 简单的并行计算
  - 需要连续内存访问的场合

- 2D 适用场景
  - 图像处理
  - 矩阵运算
  - 2D网格模拟

- 3D 适用场景
  - 体积渲染
  - 3D物理模拟
  - 体素处理

### 性能考虑
- 内存访问模式
  - 1D通常提供最佳的内存合并访问
  - 高维度可能导致内存访问不连续
  - 需要权衡问题特性和内存效率

- 线程组织
  - 选择合适的维度简化索引计算
  - 避免过度复杂的索引转换
  - 考虑warp边界对齐
